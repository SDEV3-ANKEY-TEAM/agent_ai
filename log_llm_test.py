#!/usr/bin/env python3
"""
enhanced_sequential_agent.py

â€“ ë”ë¯¸ DBì—ì„œ íŠ¸ë ˆì´ìŠ¤ ì½ê¸°
â€“ ì¤‘ë³µ ì œê±°
â€“ IsolationForest ML í•„í„°ë¡œ ê³ ìœ„í—˜ í›„ë³´ ì„ ì •
â€“ Chroma ë²¡í„° ìŠ¤í† ì–´ë¡œ ê³¼ê±° ë¡œê·¸ì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (RAG)
â€“ Gemma 2B ëª¨ë¸ë¡œ í•œ ë¬¸ì¥ ìš”ì•½
â€“ GPT-4o-minië¡œ notify/ignore ê²°ì •
â€“ Slack Webhookìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
"""

""" 
requirements.txt

pip install \
  numpy \
  python-dotenv \
  scikit-learn \
  sentence-transformers \
  chromadb \
  langchain-ollama \
  langchain-openai \
  requests

"""


import os, json, sqlite3, requests, re
from datetime import datetime
import numpy as np
from dotenv import load_dotenv

# ml / embedding
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import IsolationForest
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions

# LLM
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI


# í™˜ê²½ ë³€ìˆ˜ & ì„¤ì •
load_dotenv()
DB_PATH = "logs.db"
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Gemma 2B ì–‘ìí™” ëª¨ë¸ (ë¡œì»¬ Ollama ì„œë²„ì—ì„œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨ / ë”°ë¡œ í„°ë¯¸ë„ ë„ì›Œë†“ê³  ì‹¤í–‰í•˜ê¸°)
gemma = Ollama(model="gemma:2b-instruct-q4_0", temperature=0)
# GPT-4o-mini (OpenAI API)
gpt = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)


# Chroma ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
def init_rag_store():
    # chromadb ë²„ì „ë³„ë¡œ encode_kwargsê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ
    # ìš°ì„  model_nameë§Œ ë„˜ê¸°ê¸°
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="all-MiniLM-L6-v2"
    )
    client = chromadb.Client()
    return client.get_or_create_collection("logs", embedding_function=ef)


# ë”ë¯¸ db ì´ˆê¸°í™”
def init_dummy_db():
    if os.path.exists(DB_PATH):
        return
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute(
        """CREATE TABLE traces (
                    id   INTEGER PRIMARY KEY,
                    ts   TEXT,  
                    host TEXT,
                    msg  TEXT
                )"""
    )
    demo = [
        ("2025-06-12 10:01:00", "srv1", "sshd: Failed password for root from 1.2.3.4"),
        ("2025-06-12 10:05:03", "srv2", "nginx: GET /etc/passwd HTTP/1.1"),
        ("2025-06-12 10:07:10", "srv3", "user bob executed sudo su"),
        ("2025-06-11 09:00:00", "srv1", "sshd: Accepted password for alice"),
        ("2025-06-11 09:05:00", "srv2", "nginx: GET /index.html HTTP/1.1"),
    ]
    cursor.executemany("INSERT INTO traces(ts,host,msg) VALUES (?,?,?)", demo)
    conn.commit()
    conn.close()


def fetch_traces(limit=10):  # ìµœëŒ€ 10ê°œì˜ íŠ¸ë ˆì´ìŠ¤
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    rows = conn.execute(
        "SELECT * FROM traces ORDER BY ts DESC LIMIT ?", (limit,)
    ).fetchall()
    conn.close()
    return [dict(r) for r in rows]


def dedup(traces):
    seen, out = set(), []
    for t in traces:
        sig = hash((t["host"], t["msg"]))
        if sig not in seen:
            seen.add(sig)
            out.append(t)
    return out


# ì „ì²˜ë¦¬ + ML í•„í„°ë§ (IsolationForest)
def preprocess(msg):
    # ìˆ«ì/IP ì£¼ì†Œë¥¼ <*>ë¡œ ì¹˜í™˜
    return re.sub(r"\b[\d\.]+\b", "<*>", msg)


def filter_anomalies(traces, contamination=0.3):
    # ë©”ì‹œì§€ ì „ì²˜ë¦¬
    msgs = [preprocess(t["msg"]) for t in traces]
    # TF-IDF ë²¡í„°í™”
    vect = TfidfVectorizer()
    X = vect.fit_transform(msgs).toarray()
    # IsolationForest ìŠ¤ì½”ì–´
    clf = IsolationForest(contamination=contamination, random_state=0)
    clf.fit(X)
    scores = -clf.decision_function(X)
    thr = np.percentile(scores, 100 * (1 - contamination))
    # ì„ê³„ì¹˜ ì´ìƒì¸ ê²ƒë§Œ ë¦¬í„´
    return [traces[i] for i, s in enumerate(scores) if s >= thr]


# chroma (ì¼ì¢…ì˜ ë²¡í„° ìŠ¤í† ì–´) ë¡œ ê³¼ê±° ë¡œê·¸ ê²€ìƒ‰í•˜ê¸°
# rag  í™œìš©!
def add_to_rag(col, all_traces):
    # ì „ì²´ ê³¼ê±° ë¡œê·¸ -> ì»¬ë ‰ì…˜ì— ì¶”ê°€
    docs, metas, ids = [], [], []
    for trace in all_traces:
        docs.append(preprocess(trace["msg"]))
        metas.append({"ts": trace["ts"], "host": trace["host"]})
        ids.append(str(trace["id"]))
    col.add(documents=docs, metadatas=metas, ids=ids)


def retrieve_context(col, candidates, k=3):
    # í›„ë³´ ë¡œê·¸ë¥¼ í•˜ë‚˜ì˜ ì¿¼ë¦¬ ë¬¸ìì—´ë¡œ í•©ì³ì„œ ê²€ìƒ‰
    query = " ".join(preprocess(t["msg"]) for t in candidates)
    result = col.query(query_texts=[query], n_results=k)
    return result["documents"][0]  # ë¦¬ìŠ¤íŠ¸ of strings


# llm ìš”ì•½ ë° íŒë‹¨
def llm_summarize(traces, context=None):
    text = "\n".join(f'{t["host"]}: {t["msg"]}' for t in traces)
    if context:
        prompt = (
            f"ê³¼ê±° ìœ ì‚¬ ë¡œê·¸:\n{context}\n\nìƒˆ ë¡œê·¸:\n{text}\nâ†’ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
        )
    else:
        prompt = f"ë‹¤ìŒ ë¡œê·¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n{text}"
    return gemma.invoke(prompt).strip()


def llm_decide(summary):
    system = (
        "ë„ˆëŠ” SOC ìë™í™” ì—ì´ì „íŠ¸ë‹¤. ë‹¤ìŒ í˜•ì‹ JSONë§Œ ë°˜í™˜:\n"
        '{ "action": "notify|ignore", "severity":"low|medium|high", "reason":"15ì ì´ë‚´" }'
    )
    resp = gpt.invoke(
        [
            {"role": "system", "content": system},
            {"role": "user", "content": f"ìš”ì•½: {summary}"},
        ]
    ).content
    try:
        return json.loads(resp)
    except:
        return {"action": "ignore", "severity": "low", "reason": "jsonì˜¤ë¥˜"}


# slack ì•Œë¦¼
def notify_slack(decision, summary):
    msg = f"ğŸš¨ *{decision['severity'].upper()}* â€“ {decision['reason']}\n```{summary}```"
    res = requests.post(SLACK_WEBHOOK, json={"text": msg})
    return res.status_code == 200


# íŒŒì´í”„ë¼ì¸
def run_pipeline():
    init_dummy_db()

    # íŠ¸ë ˆì´ìŠ¤ ë¡œë“œ í›„ ì¤‘ë³µ ì œê±°
    traces = dedup(fetch_traces(limit=10))
    if not traces:
        print("ë¡œê·¸ ì—†ìŒ")
        return

    # ml í•„í„°ë§ í›„ ì´ìƒí–‰ìœ„ í›„ë³´ ì„ ì •ì •
    candidates = filter_anomalies(traces, contamination=0.3)
    if not candidates:
        print("ì´ìƒ í›„ë³´ ì—†ìŒ")
        return

    # rag ìŠ¤í† ì–´ ì´ˆê¸°í™” & ê³¼ê±° ë¡œê·¸ ì ì¬
    global RAG_COL
    try:
        RAG_COL  # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš°
    except NameError:
        RAG_COL = init_rag_store()
        add_to_rag(RAG_COL, dedup(fetch_traces(limit=1000)))  # ê³¼ê±° 1,000ê±´ ë“±ë¡

    # í›„ë³´ -> ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    context = retrieve_context(RAG_COL, candidates, k=3)

    # ìš”ì•½ í›„ íŒë‹¨
    summary = llm_summarize(candidates, context=context)
    decision = llm_decide(summary)
    print("ìš”ì•½:", summary)
    print("íŒë‹¨:", decision)

    # ì•Œë¦¼
    if decision.get("action") == "notify":
        ok = notify_slack(decision, summary)
        print("Slack ì „ì†¡:", ok)
    else:
        print("ì•Œë¦¼ ë¶ˆí•„ìš”")


if __name__ == "__main__":
    start = datetime.now()
    run_pipeline()
    print("ì†Œìš”ì‹œê°„:", datetime.now() - start)
